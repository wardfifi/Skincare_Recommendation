---
title: "skincare"
author: "Christina Pham"
date: "2025-01-30"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(httr)
library(rvest)
library(jsonlite)
library(tidyverse)
library(reticulate)
library(dplyr)
library(stringr)
```

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd

sitemap_url = "https://www.sephora.com/sitemaps/products-sitemap_en-CA.xml"
```

```{python}
from selenium import webdriver
import time

# Initialize Selenium WebDriver
#options = webdriver.ChromeOptions()
#options.add_argument("--disable-gpu")
#driver = webdriver.Chrome(options = options)  # Ensure you have ChromeDriver installed
#driver.get(sitemap_url)

#time.sleep(5)

# Get the page source after fully loading
#sitemap_html = driver.page_source

#driver.quit()

#Load html file
with open("sitemap.html", "r", encoding="utf-8") as file:
    sitemap_html = file.read()
```

```{r}
#save sitemap
sitemap_html <- py$sitemap_html
writeLines(sitemap_html, "data/sitemap.html")
```

```{python}
soup = BeautifulSoup(sitemap_html, 'html.parser')
rows = soup.select('loc')
rows = pd.DataFrame(rows)

rows
```

```{r}
rows_html <- py$rows
colnames(rows_html) <- c("link")

categories <- list(
  Cleanser = c('cleanser', 'cleanse', 'cleansing'),
  Cream = c('cream','creme','moisturizer'),
  Toner = c('toner'),
  Serum = c('serum', 'essence'),
  Exfoliator = c('exfoliant', 'exfoliate', 'exfoliators'),
  Mask = c('mask')
)
categorized_links <- list()

for (category in names(categories)) {
  keywords <- categories[[category]]
    filtered_links <- rows_html %>% 
    filter(
      sapply(keywords, function(keyword) str_detect(link, keyword)) %>% 
        rowSums() > 0
    )
    categorized_links[[category]] <- filtered_links
}

combined_df <- do.call(rbind, lapply(names(categorized_links), function(category) {
  data <- categorized_links[[category]]
  
  if (nrow(data) == 0) {
    return(data.frame(link = character(0), category = character(0)))  # Handle empty case
  }
  
  data$category <- category
  return(data)
}))

# Pass the combined DataFrame to Python
py$categorized_df <- combined_df

```

```{python}
import pandas as pd
print(categorized_df)
```


```{python}
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By  # Import By 
from selenium.webdriver.common.keys import Keys  # Import Keys for scrolling
import time
import re

#Testing
test_links = categorized_df['link'][1:2]

def scrollDown(driver, n_scroll):
    elem = driver.find_element(By.TAG_NAME, "html")
    while n_scroll >= 0:
        elem.send_keys(Keys.PAGE_DOWN)
        n_scroll -= 1
    return driver

# Setup Chrome options
options = Options()
options.add_argument("--disable-gpu")

# options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.90 Safari/537.36")
options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.90 Safari/537.36") #Christina's agent 
options.add_argument("--disable-blink-features=AutomationControlled")

# Initialize the WebDriver
driver = webdriver.Chrome(options=options)  # Ensure ChromeDriver is installed and in PATH

# Loop through each link in categorized_df['link']
products_list = []
for link in test_links:
    try:
        driver.get(link)
        time.sleep(10)  # Give page time to load
        
        #Check link if the page redirects to "productnotcarried"
        if "/search?" in driver.current_url:
          print(f" Skipping unavailable product: {link}")
          continue
        
        while True:
            browser = scrollDown(driver, 20) #scroll down the page
            time.sleep(10) #give it time to load
            break
        
        #Parse Page Source
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # Extract product name
        prod_name_element = soup.find('span', {'data-at': 'product_name'})  # Use find instead of find_all

        if prod_name_element:
          prod_name = prod_name_element.text.strip()  # Extract text
          prod_name = " ".join(word for word in prod_name.split() if word.lower() != "hair")
        else:
          prod_name = "N/A"  # Default value if not found

        # Extract brand name
        prod_element = soup.find('a', class_=['css-1kj9pbo e15t7owz0', 'css-wkag1e e15t7owz0'])
        brand_name = prod_element.text.strip() if prod_element else "N/A"

        # Removed 'size' from string. 
        #Extract brand size
        size = soup.find(['span', 'div'], class_ = ['css-15ro776', 'css-1wc0aja e15t7owz0'])
        if size:
          prod_size = size.text.strip()   # Extract text 
          prod_size = prod_size.replace('Size:', '').replace('Size', '').strip()  # Remove "Size" and extra details
        else:
          prod_size = "N/A"  # Default if not found
          
        #Product type
        category = categorized_df.loc[categorized_df['link'] == link, 'category'].values
        category = category[0] if len(category) > 0 else "N/A"  
        
        #Extract product price
        price = soup.find('b', class_='css-0')
        prod_price = price.text.strip() if price else "N/A"
        
        #Extract product rating 
        #rating = soup.find_all("span", attrs={"class": "css-egw4ri e15t7owz0"})
        # rating = soup.find_all("span", class_=re.compile(".*css-egw4ri.*e15t7owz0.*"))
        rating = soup.find_all('span', class_ = 'css-egw4ri e15t7owz0')
        if rating and len(rating) > 0:
          prod_rating = rating[0].text.strip()  # Get first match
        else:
          ratings_section = soup.find('h2', {'data-at': 'ratings_reviews_section'})
          if ratings_section and "(0)" in ratings_section.text:
            prod_rating = "0"
          else:
            prod_rating = "N/A" 
        
        #Extract brand reviews
        review = soup.find_all('span', class_ = 'css-1dae9ku e15t7owz0')
        if review and len(review) > 0:
          prod_reviews = review[0].text.strip()  # Get full text
          prod_reviews = prod_reviews.replace(",", "")  # Remove commas if present
          match = re.search(r'\d+', prod_reviews)  # Extract only the first number
          prod_reviews = match.group(0) if match else "N/A"  # Get the matched number
        else:
          # Check for "Ratings & Reviews (0)" when no reviews exist
          ratings_section = soup.find('h2', {'data-at': 'ratings_reviews_section'})
          if ratings_section and "(0)" in ratings_section.text:
            prod_reviews = "0"
          else:
            prod_reviews = "N/A"

        #Extract Description
        time.sleep(3)
        # Locate all divs that may contain product descriptions
        description_classes = ['css-1v2oqzv e15t7owz0', 'css-1j9v5fd e15t7owz0', 'css-1uzy5bx e15t7owz0', 'css-12cvig4 e15t7owz0', 'css-eccfzi e15t7owz0', 'css-11gp14a e15t7owz0', 'css-2f6kh5 e15t7owz0']

        description_tags = ['p', 'b', 'strong']

        # Initialize default value
        prod_desc = "N/A"

        for class_name in description_classes:
          divs = soup.find_all('div', class_=class_name)

          for div in divs:
            for tag in description_tags:
              element = div.find(tag, string=lambda text: text and "What it is:" in text)
              # element = div.find(tag)
              # Check if the element contains "What it is:"
              if element: 

                # Extract text while handling possible formatting issues
                extracted_text = element.get_text(separator=" ", strip=True).replace("What it is:", "").strip()

                # Case 2: The description follows the tag as a sibling text
                if not extracted_text and element.next_sibling:
                  extracted_text = element.next_sibling.strip()
    
                # Case 3: The description is inside a `<p>` tag after the strong/b tag if not extracted_text:
                if not extracted_text:
                  next_container = element.find_next_sibling("p")
                  if next_container:
                    extracted_text = next_container.get_text(strip=True)
                    
                # **Case 4: The description is inside a `<div>` right after**
                if not extracted_text:
                  next_div = element.find_next_sibling("div")
                  if next_div:
                    extracted_text = next_div.get_text(strip=True)

                # If valid text is found, set it and stop searching
                if extracted_text:
                  prod_desc = extracted_text
                  break # Stop searching once we find a valid description

            if prod_desc != "N/A":
              break  # Stop checking other divs once we get the correct description

        # Print the extracted product description
        print(f"Product Description: {prod_desc}")


        #Extract Skin Types
        description_classes2 = ['css-1v2oqzv e15t7owz0', 'css-1j9v5fd e15t7owz0', 'css-1uzy5bx e15t7owz0', 'css-12cvig4 e15t7owz0', 'css-eccfzi e15t7owz0', 'css-11gp14a e15t7owz0', 'css-2f6kh5 e15t7owz0']

        description_tags2 = ['p', 'b', 'strong']

        # Initialize default value
        skin_type = "N/A"

        for class_name in description_classes2:
          divs = soup.find_all('div', class_=class_name)

          for div in divs:
            for tag in description_tags2:
              element = div.find(tag, string=lambda text: text and ("Skin Type:" in text or "Skin Types:" in text or "Skincare Type:" in text or "Skincare Types:" in text))
              if element: 

                # Extract text while handling possible formatting issues
                extracted_text = element.get_text(separator=" ", strip=True).replace("Skincare Types:", "").replace("Skincare Type:", "").replace("Skin Type:", "").replace("Skin Types:","").strip()

                # Case 2: The description follows the tag as a sibling text
                if not extracted_text and element.next_sibling:
                  extracted_text = element.next_sibling.strip()
    
                # Case 3: The description is inside a `<p>` tag after the strong/b tag if not extracted_text:
                if not extracted_text:
                  next_container = element.find_next_sibling("p")
                  if next_container:
                    extracted_text = next_container.get_text(strip=True)
                    
                # **Case 4: The description is inside a `<div>` right after**
                if not extracted_text:
                  next_div = element.find_next_sibling("div")
                  if next_div:
                    extracted_text = next_div.get_text(strip=True)

                # If valid text is found, set it and stop searching
                if extracted_text:
                  skin_type = extracted_text
                  break # Stop searching once we find a valid description

            if skin_type != "N/A":
              break  # Stop checking other divs once we get the correct description
            
        # element_types = soup.find_all('div', class_ = ['css-1v2oqzv e15t7owz0', 'css-1j9v5fd e15t7owz0', 'css-1uzy5bx e15t7owz0', 'css-12cvig4 e15t7owz0', 'css-11gp14a e15t7owz0', 'css-2f6kh5 e15t7owz0'])
        # 
        # skin_type = "N/A"
        # 
        # for div in element_types:
        #   strong_elements = div.find_all(['strong', 'b'])  # Find all <strong> and <b> elements in this div
        # 
        #   for tag in strong_elements:
        #     if "Skin Type:" in tag.text:
        #       next_text = tag.next_sibling
        #       if next_text and next_text.strip():
        #         skin_type = next_text.strip()
        #       else:
        #         next_container = tag.find_next_sibling()  # Backup check for next sibling
        #         if next_container:
        #           skin_type = next_container.get_text(strip=True)

        #Extract Concerns
        description_classes3 = ['css-1v2oqzv e15t7owz0', 'css-1j9v5fd e15t7owz0', 'css-1uzy5bx e15t7owz0', 'css-12cvig4 e15t7owz0', 'css-eccfzi e15t7owz0', 'css-11gp14a e15t7owz0', 'css-2f6kh5 e15t7owz0']

        description_tags3 = ['p', 'b', 'strong']

        # Initialize default value
        skin_concerns = "N/A"

        for class_name in description_classes3:
          divs = soup.find_all('div', class_=class_name)

          for div in divs:
            for tag in description_tags3:
              element = div.find(tag, string=lambda text: text and ("Skincare Concerns:" in text or "Skincare Concern:" in text))
              if element: 

                # Extract text while handling possible formatting issues
                extracted_text = element.get_text(separator=" ", strip=True).replace("Skincare Concern:", "").replace("Skincare Concerns:", "").replace("- ", "").strip()

                # Case 2: The description follows the tag as a sibling text
                if not extracted_text and element.next_sibling:
                  extracted_text = element.next_sibling.strip()
    
                # Case 3: The description is inside a `<p>` tag after the strong/b tag if not extracted_text:
                if not extracted_text:
                  next_container = element.find_next_sibling("p")
                  if next_container:
                    extracted_text = next_container.get_text(strip=True)
                    
                # **Case 4: The description is inside a `<div>` right after**
                if not extracted_text:
                  next_div = element.find_next_sibling("div")
                  if next_div:
                    extracted_text = next_div.get_text(strip=True)

                # If valid text is found, set it and stop searching
                if extracted_text:
                  skin_concerns = extracted_text
                  break # Stop searching once we find a valid description

            if skin_concerns != "N/A":
              break  # Stop checking other divs once we get the correct description
            
        # element_concerns = soup.find_all('div', class_ = ['css-1v2oqzv e15t7owz0', 'css-1j9v5fd e15t7owz0', 'css-1uzy5bx e15t7owz0', 'css-12cvig4 e15t7owz0', 'css-eccfzi e15t7owz0', 'css-11gp14a e15t7owz0', 'css-2f6kh5 e15t7owz0'])
        # 
        # # Initialize default values
        # skin_concerns = "N/A"
        # 
        # for div in element_concerns:
        #   strong_elements = div.find_all(['strong','b'])  # Find all <strong> elements in this div
        # 
        #   for tag in strong_elements:
        #     if "Skincare Concerns:" in tag.text:
        #       next_text = tag.next_sibling  # Get text after <b> or <strong>
        #       if next_text and next_text.strip():
        #         skin_concerns = next_text.strip().replace("-", "")
        #       else:
        #         next_container = tag.find_next_sibling()  # Backup check for next sibling
        #         if next_container:
        #           skin_concerns = next_container.get_text(strip=True).replace("-", "")
        # 
        # 
        #   # Handle cases where concerns are listed inside separate `<p>` tags
        #   p_tags = div.find_all('p')
        #   p_concerns = [p.text.strip().replace("-", "").strip() for p in p_tags if p.text.strip()]
        #   # If `<p>` contains concerns, update values
        #   if p_concerns:
        #     if "Skincare Concerns:" in strong_elements:
        #       skin_concerns = ", ".join(p_concerns)  # Join multiple concerns
              
        #Extract Ingredients
        ingredient_element = soup.find('div', class_ = 'css-1mb29v0 e15t7owz0')
        if ingredient_element:
          prod_ingredients = ingredient_element.text.strip()
        else:
          prod_ingredients = 'N/A'
  
        # Append data
        products_list.append({"Brand Name": brand_name,
                              "Product Name": prod_name,
                              "Product Category": category,
                              "Product Price": prod_price,
                              "Product Rating": prod_rating,
                              "Product Size": prod_size,
                              "Product Reviews": prod_reviews,
                              "Product Description": prod_desc,
                              "Product Ingredients": prod_ingredients,
                              "Skin Type": skin_type,
                              "Skin Concerns": skin_concerns,
                              "URL": link})
        #check if it processes link 
        print(f" processed: {link}")

    except Exception as e:
        print(f" Error processing {link}: {e}")

# Close WebDriver *after* processing all links
driver.quit()

product_df = pd.DataFrame(products_list)
print(product_df)
View(product_df)
```


```{python}
import pandas as pd
import os

# Save pandas dataframe to an xlsx file. **[replace with your own path**]
#product_df.to_excel("/Users/choco/OneDrive/Documents/GitHub/Skin_Care_Recommendation/Christina246-286.xlsx", index = False)

```

